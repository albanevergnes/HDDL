{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0042362a",
   "metadata": {},
   "source": [
    "## Importation de librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02b7d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #GPU si on est au gmm sinon cpu = lent\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bcc11",
   "metadata": {},
   "source": [
    "## Téléchargement du dataset FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cdc03-e76d-4ca7-ad29-d029e5522f33",
   "metadata": {},
   "source": [
    "The ten categories in Fashion MNIST are:\n",
    "1.T-shirt/top\n",
    "2.Trouser\n",
    "3.Pullover\n",
    "4.Dress\n",
    "5.Coat\n",
    "6.Sandal\n",
    "7.Shirt\n",
    "8.Sneaker\n",
    "9.Bag\n",
    "10.Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67991533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "batch_size = 128\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.FashionMNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c111b-992d-491e-9ae4-7b763813574a",
   "metadata": {},
   "source": [
    "### Def : (que j'avais pas compris)\n",
    "\n",
    "Un batch, c’est un groupe d’images traitées en même temps.  \n",
    "Exemple : batch size = 32, le réseau voit 32 images en parallèle.\n",
    "\n",
    "kernel (ou filtre) =/ de batch : c'est un petit détecteur de motifs.  \n",
    "il glisse sur l’image, il regarde une petite zone locale, il produit une activation\n",
    "\n",
    "Canal :  \n",
    "Chaque canal représente un motif différent, appris par un filtre différent.  \n",
    "Attention, un canal peut avoir des valeurs grandes et un autre très petites.\n",
    "\n",
    "Tenseur :  \n",
    "un tableau de normes 3D, 4D etc  \n",
    "ici image MNIST : [1, 28, 28],  \n",
    "batch de 64 images : [64, 1, 28, 28]  \n",
    "=> Tenseur en 4D\n",
    "\n",
    "Activation :  \n",
    "valeur produite après une couche (conv, linéaire etc)  \n",
    "entrée-> convolution -> activation  \n",
    "Chaque pixel de sortie de conv est une activation\n",
    "\n",
    "Rappel :  \n",
    "L’encodeur prend une image + son label  \n",
    "il les compresse en une représentation latente probabiliste, conditionnée par la classe\n",
    "\n",
    "---\n",
    "\n",
    "### Explications :\n",
    "\n",
    "_init_ :  \n",
    "definit l'architecture de l'encodeur et du décodeur  \n",
    "(Latent_dim = taille du vecteur latent z, num_classes= nombre de classes ex robe, t-shirt mais c'est des chiffres)\n",
    "\n",
    "---\n",
    "\n",
    "### Partie Encodeur :\n",
    "\n",
    "self.encodeur :  \n",
    "recoit l'image + notre condition (label) qui sont concaténés en canaux  \n",
    "\n",
    "(Le label est transformé en cartes spatiales constantes :  \n",
    "chaque pixel “sait” à quelle classe appartient l’image  \n",
    "donc pour l’encodeur, le label est juste une information supplémentaire par pixel.)\n",
    "\n",
    "Puis 3 convolutions réduisant la taille (réduire les détails inutiles)  \n",
    "et en augmentant les canaux (on augmente la richesse de l'information)  \n",
    "grace à torch.nn via la focntion Conv2d.\n",
    "\n",
    "Pour chaque Convolution, les filtres voient image + classe  \n",
    "et apprennent les motifs importants pour la classe.\n",
    "\n",
    "Donc 1 + num_classes c'est les canaux du début de l'encodeur,  \n",
    "32 le nombre de canaux que l'on veut à la fin de la première couche  \n",
    "etc jusqu'à 128.\n",
    "\n",
    "---\n",
    "\n",
    "#### remarques : comment ca marche vraiment ?\n",
    "\n",
    "nn.Sequential est un conteneur de couches :  \n",
    "Conv2d -> BatchNorm2d -> ReLU  \n",
    "en cycle pour chaque couche du réseau\n",
    "\n",
    "D'abord on fait la convolution avec Conv2d  \n",
    "(couche de convolution 2D):  \n",
    "repére des motifs visuels (bords, courbes, formes)  \n",
    "et produire des cartes de caractéristiques (features).\n",
    "\n",
    "On lui donnes le batch_size, le nombre de canaux  \n",
    "et la hauteur/largeur des images  \n",
    "==> Tenseur de 4D.\n",
    "\n",
    "Attention : choix de modélisation\n",
    "\n",
    "Le kernel, c’est la fenêtre de vision du filtre,  \n",
    "kernel = 4 → le filtre regarde un bloc 4×4.\n",
    "\n",
    "Le stride dit de combien de pixels le filtre avance à chaque pas.  \n",
    "stride = 1 → pixel par pixel  \n",
    "stride = 2 → un pixel sur deux\n",
    "\n",
    "Le padding ajoute une bordure de zéros autour de l’image.  \n",
    "Sans padding les bords sont moins vus et l’image rétrécit trop vite.  \n",
    "Avec padding = 1, les filtres peuvent encore “voir” les bords  \n",
    "et la réduction est contrôlée.\n",
    "\n",
    "Attention :  \n",
    "Pourquoi je ne donne pas la taille de l’image à Conv2d ?  \n",
    "car une convolution est locale et indépendante de la taille globale de l’image  \n",
    "vu que le kernel regarde une petite zone locale,  \n",
    "il se déplace automatiquement sur toute l’image,  \n",
    "peu importe que l’image fasse 28×28 ou 256×256.\n",
    "\n",
    "---\n",
    "\n",
    "pour chaque convolution (couches),  \n",
    "on utilise BatchNorm2d stabiliser et accélérer le train  \n",
    "(sinon les gradients sont instables par ex).\n",
    "\n",
    "La fonction travaille canal par canal (pas par pixel)  \n",
    "ou il regarde tous les pixels,  \n",
    "de toutes les images du batch,  \n",
    "mais pour chaque canal indépendament.\n",
    "\n",
    "Il force les activations à rester dans une plage “raisonnable”  \n",
    "en centrant les valeurs autour de 0  \n",
    "(les recentre (moyenne ≈ 0) et les redimensionne (variance ≈ 1) pour VAE).\n",
    "\n",
    "=> BatchNorm normalise chaque canal séparément.\n",
    "\n",
    "Il apprend :  \n",
    "γ (gamma) : échelle  \n",
    "β (beta) : décalage  \n",
    "\n",
    "Formule finale :  \n",
    "y=γx+β\n",
    "\n",
    "Une activation instable est une valeur num qui pose problème  \n",
    "pendant l'entrainement\n",
    "\n",
    "Ex :  \n",
    "activation trop grande comme  \n",
    "[ 0.5, 1.2, 3.8, 57.3, 1200.7 ]  \n",
    "=> gradients énormes, poids qui explosent, perte = NaN  \n",
    "=> entraînement impossible\n",
    "\n",
    "activation trop petite comme  \n",
    "[ 0.00001, 0.00002, 0.00000 ]  \n",
    "=> gradients ≈ 0, le réseau n’apprend plus  \n",
    "=> apprentissage bloqué\n",
    "\n",
    "---\n",
    "\n",
    "ReLU(Rectified Linear Unit)  \n",
    "permet la non-linéarité  \n",
    "donc permets d'apprendre des formes complexes :\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "En gros agit comme un intérrupteur  \n",
    "signal utile → passe  \n",
    "signal inutile → coupé\n",
    "\n",
    "Finalement,  \n",
    "BatchNorm centre valeurs autour de 0  \n",
    "(les recentre (moyenne ≈ 0) et les redimensionne (variance ≈ 1) pour VAE)  \n",
    "et ReLU supprime les valeurs négatives  \n",
    "=> le réseau apprend quand activer ou non un neurone\n",
    "\n",
    "---\n",
    "\n",
    "### PARTIE ESPACE LATENT : (Sampling)\n",
    "\n",
    "Attention, un VAE ne sort pas une image,  \n",
    "il sort une distribution latente q(z∣x,c)  \n",
    "définie par une moyenne mue et une variance qui sont des vecteurs.\n",
    "\n",
    "Mais après la partie décodeur,  \n",
    "on nous donne à la fin [Batch, 128, 4, 4]  \n",
    "donc chiant aussi  \n",
    "on va transformer transforme la “petite image de features”  \n",
    "en un vecteur latent avec X.view.\n",
    "\n",
    "Ensuite on fait nn.linear  \n",
    "ou chaque sortie dépend de toutes les entrées,  \n",
    "contrairement à une convolution (locale),  \n",
    "on change de type de couche  \n",
    "(on enlève la dimension spatiale, on veut des vecteurs globaux).\n",
    "\n",
    "Ici on veut passer de la taille du vecteur après l'encodeur  \n",
    "128*4*4  \n",
    "à la taille de la dim latente (dimension fixée).\n",
    "\n",
    "On fait bien deux nn.linear  \n",
    "car on veut récupérer le vecteur des mu et des logvar.\n",
    "\n",
    "Attention :  \n",
    "c'est bien logvar et pas variance directement  \n",
    "car la variance doit être positive  \n",
    "et que on ne sait pas quel nombre peut sortir du réseau  \n",
    "(on fait plustard exp(logvar) via la fonction sample).\n",
    "\n",
    "Attention :  \n",
    "c_mu et fc_logvar sont identiques au départ,  \n",
    "leur rôle est défini par leur utilisation dans le sampling  \n",
    "et la KL divergence dans la loss (voir focntion loss_function)  \n",
    "donc c'est le réseau apprend à leur donner un sens.\n",
    "\n",
    "---\n",
    "\n",
    "### Partie Decodage :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b635504",
   "metadata": {},
   "source": [
    "## 1. Definition de la classe CVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes=10): \n",
    "        super(ConvCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # --- ENCODER ---\n",
    "\n",
    "        # Le premier Conv2d prend 1 + 10 = 11 canaux (1 canal pour l'image N&B + 10 canaux pour les labels one-hot étirés)\n",
    "        # a chaque étape de l'encodeur on divise la taille de l'image et augmente la taille des caractéristiques \n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1 + num_classes, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, 14, 14) #stride=2 divise la taille de l'image en 2\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 7, 7)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Output: (128, 4, 4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Transition entre le monde des convolutions et l'espace latent\n",
    "        # On applatit le volume de sorties de l'encodeur (qui est en 3D)\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim) #Prédit la moyenne (μ) de la distribution pour chaque dimension de l'espace latent.\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim) #Prédit le logarithme de la variance (log(σ2)) pour chaque dimension.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- DECODER ---\n",
    "        #  L'entrée du décodeur prend Latent + Label\n",
    "        self.fc_decode = nn.Linear(latent_dim + num_classes, 128 * 4 * 4) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    # Fusion image + Label et encodage\n",
    "    def encode(self, x, c): \n",
    "        # x: [Batch, 1, 28, 28] (image)\n",
    "        # c: [Batch, 10] (One-hot labels)\n",
    "        \n",
    "        # Astuce : On étire le vecteur label pour qu'il fasse la taille de l'image (28x28)\n",
    "        # c devient [Batch, 10, 28, 28]\n",
    "        c_expanded = c.view(-1, self.num_classes, 1, 1).expand(-1, -1, 28, 28)\n",
    "        \n",
    "        # On concatène sur la dimension des canaux (dim=1)\n",
    "        # inputs devient [Batch, 11, 28, 28]\n",
    "        inputs = torch.cat([x, c_expanded], 1)\n",
    "        \n",
    "        x = self.encoder(inputs)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    # Tirage aléatoire de z dans l'espace latent\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    #Reconstruction\n",
    "    def decode(self, z, c):\n",
    "        # z: [Batch, latent_dim]\n",
    "        # c: [Batch, num_classes]\n",
    "        \n",
    "        #ce sont deux vecteurs, on les colle\n",
    "        inputs = torch.cat([z, c], 1) \n",
    "        \n",
    "        x = self.fc_decode(inputs)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    #Flux principal pour enchainer les différents actions\n",
    "    def forward(self, x, labels):\n",
    "        \n",
    "        # 1. Création du One-Hot Vector pour les labels\n",
    "        c = torch.zeros(x.size(0), self.num_classes).to(x.device)\n",
    "        c.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # 2. Encodage (Image + Condition)\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        \n",
    "        # 3. Sampling\n",
    "        z = self.sample(mu, logvar)\n",
    "        \n",
    "        # 4. Décodage (Latent + Condition)\n",
    "        recon_x = self.decode(z, c)\n",
    "        \n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de perte\n",
    "def loss_function(recon_x, x, mu, logvar, beta):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + beta * KLD,BCE, KLD\n",
    "\n",
    "\n",
    "# Le paramètre beta permet de pondérer l'importance de la KLD par rapport à la reconstruction (BCE)\n",
    "# Un beta élevé favorise un espace latent bien structuré (utile pour la génération).\n",
    "# Un beta faible favorise une reconstruction très fidèle (utile pour la précision des détails)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9565b",
   "metadata": {},
   "source": [
    "#### Point théorique : Adam optimizer\n",
    "Adam (Adaptive Moment Estimation) est un algorithme d'optimisation utilisé pour l'entraînement des réseaux de neurones. \n",
    "\n",
    "Il est actuellement l'un des plus populaires car il combine les avantages de deux autres extensions de la descente de gradient : Momentum et RMSProp.\n",
    "\n",
    "L'idée de Adam est de calculer un taux d'apprentissage adaptatif pour chaque paramètre du réseau. Au lieu d'utiliser un pas de mise à jour unique pour tous les poids, Adam ajuste la vitesse de modification de chaque poids en fonction de l'historique de ses gradients.\n",
    "\n",
    "Il s'appuie sur deux statistiques calculées à chaque étape t :\n",
    "\n",
    "- Le premier moment (mt​) : C'est la moyenne mobile des gradients (similaire au Momentum). Cela permet de garder une direction cohérente et d'éviter les oscillations brutales.\n",
    "\n",
    "- Le second moment (vt​) : C'est la moyenne mobile du carré des gradients (similaire à RMSProp). Cela permet de normaliser les mises à jour en fonction de la variance des gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86dccc",
   "metadata": {},
   "source": [
    "## 2. Entrainement du CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e36883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters arbitraires\n",
    "batch_size = 128\n",
    "latent_dim = 10\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "beta = 1\n",
    "\n",
    "\n",
    "# Initialisation du CVAE model et du Adam optimizer\n",
    "cvae =ConvCVAE(latent_dim=latent_dim, num_classes=10).to(device) # c'est ici qu'on instance la dim latente\n",
    "cvae.to(device)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fonction pour entrainer le CVAE\n",
    "def train_cvae_model(model, train_loader, optimizer, epochs, beta, device):\n",
    "    history = {'loss': [], 'bce': [], 'kld': []}\n",
    "    \n",
    "    print(f\"Début de l'entraînement : {epochs} epochs, Beta={beta}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_bce = 0\n",
    "        epoch_kld = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            \n",
    "            # Calcul de la perte via la fonction définie précédemment\n",
    "            loss, bce, kld = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bce += bce.item()\n",
    "            epoch_kld += kld.item()\n",
    "\n",
    "        # Normalisation par le nombre total d'images\n",
    "        num_samples = len(train_loader.dataset)\n",
    "        avg_loss = epoch_loss / num_samples\n",
    "        avg_bce = epoch_bce / num_samples\n",
    "        avg_kld = epoch_kld / num_samples\n",
    "\n",
    "        # Enregistrement dans l'historique\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['bce'].append(avg_bce)\n",
    "        history['kld'].append(avg_kld)\n",
    "\n",
    "        # Affichage de la progression\n",
    "        print(f\"Epoch [{epoch}/{epochs}] | Loss: {avg_loss:.2f} | BCE: {avg_bce:.2f} | KLD: {avg_kld:.2f}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Entraînement terminé.\")\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "train1 = train_cvae_model(\n",
    "    model=cvae, \n",
    "    train_loader=train_loader, \n",
    "    optimizer=optimizer, \n",
    "    epochs=30, \n",
    "    beta=1, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff026e",
   "metadata": {},
   "source": [
    "## Visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426da810",
   "metadata": {},
   "source": [
    "#### A. Visulation des images décodées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_comparison(original_images, reconstructions, n_images=6):\n",
    "    \"\"\"Plots the original images and its reconstructions for comparison\n",
    "\n",
    "    Args:\n",
    "        original_image (torch.Tensor): The original images\n",
    "        reconstructions (torch.Tensor): Reconstruction of the original images\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2, n_images, figsize=(20, 7))\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        # Plot original images\n",
    "        ax[0, i].imshow(original_images[i], cmap='gray')\n",
    "        ax[0, i].axis('off')\n",
    "        ax[0, 0].set_title('Original')\n",
    "\n",
    "        # Reconstructed images\n",
    "        ax[1, i].imshow(reconstructions[i], cmap='gray')\n",
    "        ax[1, i].axis('off')\n",
    "        ax[1, 0].set_title('Recomstruction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 1. Récupérer un batch de données de test\n",
    "batch_images, batch_labels = next(iter(test_loader))\n",
    "\n",
    "# 2. Sélectionner un échantillon (ex: les 8 premières images)\n",
    "n_samples = 8\n",
    "inputs = batch_images[:n_samples].to(device)\n",
    "labels = batch_labels[:n_samples].to(device)\n",
    "\n",
    "# 3. Obtenir les reconstructions\n",
    "cvae.eval() # Mode évaluation (désactive BatchNorm)\n",
    "with torch.no_grad():\n",
    "    recons, _, _ = cvae(inputs, labels) #encodage et decodage des 8 images\n",
    "\n",
    "# 4. Préparation pour l'affichage (CPU + Numpy)\n",
    "# On passe de [N, 1, 28, 28] à [N, 28, 28] avec squeeze()\n",
    "original_images = inputs.cpu().numpy().squeeze()\n",
    "reconstructed_images = recons.cpu().numpy().squeeze()   \n",
    "\n",
    "image_comparison(original_images, reconstructed_images, n_images=n_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e31b6",
   "metadata": {},
   "source": [
    "On observe un plutot bonne reconstruction des images même sans optimisation spécifiques des hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fe133",
   "metadata": {},
   "source": [
    "#### B. Visualisation de l'espace latent, Partie à améliorer ou à mettre ailleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a826cd",
   "metadata": {},
   "source": [
    "Nous affichons seulement les deux premieres dimensions de l'espace latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b63f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_latent_space(model, beta):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # On prend un grand lot d'images de test pour bien voir les clusters\n",
    "        test_loader_viz = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=5000, shuffle=False)\n",
    "        data, labels = next(iter(test_loader_viz))\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Créer le vecteur One-Hot 'c' manuellement car model.encode() en a besoin\n",
    "        c = torch.zeros(data.size(0), 10).to(device)\n",
    "        c.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # 2. Encoder avec l'image ET la condition\n",
    "        mu, logvar = model.encode(data, c)\n",
    "        \n",
    "        # 3. Récupérer les coordonnées (on utilise mu, la moyenne)\n",
    "        z = mu.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # 4. Affichage\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(z[:, 0], z[:, 1], c=labels_np, cmap='tab10', alpha=0.6, s=10)\n",
    "        plt.colorbar(scatter, ticks=range(10), label='Classes (0-9)')\n",
    "        plt.title(f'Espace Latent CVAE (Beta = {beta})')\n",
    "        plt.xlabel('Dimension Latente 1')\n",
    "        plt.ylabel('Dimension Latente 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show() #\n",
    "\n",
    "plot_latent_space(cvae,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54273e",
   "metadata": {},
   "source": [
    "Tous les points sont mélangés et c'est normal pour un cvae car on lui donne les étiquettes donc les vetements sont regroupés par style de dessin et non par classe. De plus, notre espace latent a 10 dimensions donc on observe ici qu'une petite partie de ce dernier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d23518",
   "metadata": {},
   "source": [
    "# Génération de 5 nouveaux échantillons (Question 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0dd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_per_class(num_samples=5):\n",
    "    cvae.eval()\n",
    "    num_classes = 10\n",
    "    \n",
    "    # On prépare une grille de plots : 10 lignes (classes), 5 colonnes (samples)\n",
    "    fig, ax = plt.subplots(num_classes, num_samples, figsize=(num_samples * 1.5, num_classes * 1.5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            # 1. Sample random latent vectors (z)\n",
    "            # On génère 'num_samples' vecteurs de bruit\n",
    "            z = torch.randn(num_samples, cvae.latent_dim).to(device) # z, le bruit, est aléatoire \n",
    "            \n",
    "            # 2. Prepare the Condition (c)\n",
    "            # On crée les labels pour la classe actuelle (ex: [0, 0, 0, 0, 0] pour la première boucle)\n",
    "            labels = torch.tensor([class_idx] * num_samples).view(-1, 1).to(device) # on choisit la classe ( quel vetement on veut générer)\n",
    "            \n",
    "            # On convertit en One-Hot (nécessaire pour le décodeur du CVAE)\n",
    "            c = torch.zeros(num_samples, num_classes).to(device)\n",
    "            c.scatter_(1, labels, 1)\n",
    "            \n",
    "            # 3. Decode the latent vectors with the condition\n",
    "            samples = cvae.decode(z, c)\n",
    "            \n",
    "            # Reshape pour l'affichage\n",
    "            samples = samples.cpu().view(num_samples, 28, 28)\n",
    "\n",
    "            # 4. Affichage dans la grille\n",
    "            for i in range(num_samples):\n",
    "                ax[class_idx, i].imshow(samples[i], cmap='gray')\n",
    "                ax[class_idx, i].axis('off')\n",
    "                \n",
    "                # Ajout du titre seulement sur la première colonne\n",
    "                if i == 0:\n",
    "                    ax[class_idx, i].set_title(f\"Class {class_idx}\", fontsize=10, x=-0.2, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Lancer la génération\n",
    "generate_sample_per_class(num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17e0e0",
   "metadata": {},
   "source": [
    "### Optimisation des hyperparamètres "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096f862",
   "metadata": {},
   "source": [
    "Nous cherchons à optimiser le learning rate, la taille de batch, beta de la fonction de perte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d1349",
   "metadata": {},
   "source": [
    "Pour commencer, on crée un jeu de validation afin de tester nos hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148042e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test data into validation and test sets, comme dans le tp, attention ne pas run deux fois\n",
    "\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(test_dataset, [5000, 5000])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce272b",
   "metadata": {},
   "source": [
    "On utilise la classe CVAE défnie au début du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e81365",
   "metadata": {},
   "source": [
    "### Point théorique : quel ordre pour l'optimisation des hyperparametres  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c1817",
   "metadata": {},
   "source": [
    "Pour l'optimisation, Il n'existe pas de règle absolue, mais la pratique standard en deep learning suit généralement cet ordre de priorité :\n",
    "\n",
    "1. Définir une architecture de base (Baseline)\n",
    "\n",
    "Avant d'optimiser, vous devez avoir une architecture cohérente avec la complexité de votre problème.\n",
    "\n",
    "    Capacité du modèle : Si votre architecture est trop simple (ex: hidden_dim trop petit), le modèle subira un sous-apprentissage (underfitting), peu importe la qualité du learning rate.\n",
    "\n",
    "    Objectif : Atteindre un niveau de performance minimal où le modèle commence à apprendre (la perte diminue).\n",
    "\n",
    "2. Optimiser les hyperparamètres critiques (Learning Rate / Batch Size)\n",
    "\n",
    "C'est l'étape la plus cruciale après avoir fixé l'architecture de base.\n",
    "\n",
    "    Le Learning Rate (LR) : C'est l'hyperparamètre le plus important. Un mauvais LR empêchera n'importe quelle architecture de converger.\n",
    "\n",
    "    Le Batch Size : Il influence la stabilité du gradient et la vitesse d'entraînement.\n",
    "\n",
    "    Méthode : Utilisez un \"Learning Rate Finder\" ou une recherche aléatoire (Random Search) sur une échelle logarithmique.\n",
    "\n",
    "3. Ajuster l'architecture (Fine-tuning structurel)\n",
    "\n",
    "Une fois que vous avez un learning rate stable, vous pouvez affiner la structure :\n",
    "\n",
    "    Ajouter ou retirer des couches.\n",
    "\n",
    "    Modifier la dimension du width ou du hidden_dim.\n",
    "\n",
    "    Ajouter de la régularisation (Dropout, Batch Normalization) si vous observez du sur-apprentissage (overfitting).\n",
    "\n",
    "4. Itérer\n",
    "\n",
    "Le processus est cyclique. Si vous changez radicalement l'architecture (ex: doubler le nombre de couches), vous devrez souvent réajuster légèrement le learning rate.\n",
    "Résumé de la stratégie recommandée\n",
    "\n",
    "    Architecture : Concevez une structure \"raisonnable\" (comme celle de votre code).\n",
    "\n",
    "    Learning Rate : Trouvez la valeur optimale pour cette structure.\n",
    "\n",
    "    Architecture : Testez des variations de profondeur/largeur pour améliorer les résultats.\n",
    "\n",
    "    Hyperparamètres : Affinez le reste (poids, batch size, optimiseur).\n",
    "\n",
    "Conseil technique : Pour un Autoencoder, commencez toujours par vérifier si le modèle peut \"overfitter\" (mémoriser parfaitement) un très petit échantillon de données (5-10 exemples). Si l'architecture et le learning rate ne permettent pas cela, le problème est structurel ou lié au LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour optimiser les hyperparametres : batch, dim latente, learning rate et beta\n",
    "\n",
    "import itertools\n",
    "\n",
    "def hyperparameter_search(train_dataset, val_dataset, device):\n",
    "    # 1. Définition de la grille de recherche\n",
    "    param_grid = {\n",
    "        'latent_dim': [10, 20],\n",
    "        'beta': [1.0, 5.0],\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'batch_size': [128, 256, 512]\n",
    "    }\n",
    "    \n",
    "    # Génère toutes les combinaisons possibles\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    results = []\n",
    "    epochs_test = 10 \n",
    "    \n",
    "    print(f\"Total de combinaisons à tester : {len(combinations)}\")\n",
    "    \n",
    "    for i, config in enumerate(combinations):\n",
    "        print(f\"\\n--- Test {i+1}/{len(combinations)} : {config} ---\")\n",
    "        \n",
    "        # DataLoader d'entraînement\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # DataLoader de validation\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            dataset=val_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Initialisation du modèle et de l'optimiseur\n",
    "        model = ConvCVAE(latent_dim=config['latent_dim'], num_classes=10).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        \n",
    "        # Entraînement (sur le train_dataset)\n",
    "        train_cvae_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs_test,\n",
    "            beta=config['beta'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # --- Évaluation sur le val_dataset ---\n",
    "        model.eval()\n",
    "        val_bce_total = 0\n",
    "        val_kld_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                recon, mu, logvar = model(data, labels)\n",
    "                _, bce, kld = loss_function(recon, data, mu, logvar, config['beta'])\n",
    "                val_bce_total += bce.item()\n",
    "                val_kld_total += kld.item()\n",
    "        \n",
    "        # Calcul des moyennes par image pour le jeu de validation\n",
    "        avg_val_bce = val_bce_total / len(val_dataset)\n",
    "        avg_val_kld = val_kld_total / len(val_dataset)\n",
    "        \n",
    "        # Stockage des résultats de validation\n",
    "        config_results = {\n",
    "            **config,\n",
    "            'val_bce': avg_val_bce,\n",
    "            'val_kld': avg_val_kld,\n",
    "            'val_total': avg_val_bce + (config['beta'] * avg_val_kld)\n",
    "        }\n",
    "        results.append(config_results)\n",
    "        print(f\"Validation Result -> Total Loss: {config_results['val_total']:.2f}, BCE: {avg_val_bce:.2f}, KLD: {avg_val_kld:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exécution avec les deux datasets distincts\n",
    "all_results = hyperparameter_search(train_dataset, val_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri des résultats par la loss totale (ordre croissant car on minimise la perte)\n",
    "sorted_results = sorted(all_results, key=lambda x: x['val_total'])\n",
    "\n",
    "# Affichage du Top 3\n",
    "print(\"\\n--- TOP 3 DES MEILLEURES CONFIGURATIONS ---\")\n",
    "for i, res in enumerate(sorted_results[:3]):\n",
    "    print(f\"Rang {i+1} : Loss Totale = {res['val_total']:.4f}\")\n",
    "    print(f\"  Configuration : latent_dim={res['latent_dim']}, beta={res['beta']}, lr={res['lr']}, batch_size={res['batch_size']}\")\n",
    "    print(f\"  Détails : BCE={res['val_bce']:.4f}, KLD={res['val_kld']:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1a7e2",
   "metadata": {},
   "source": [
    "Après optimisation, nous gardons dim_lat=10, beta=1, lr=0.001, batch_size=128 qui sont les hyperparamètres qui minimisent le plus la perte. Nous allons maintenant chercher à optimiser l'architecture de notre CVAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594392ff",
   "metadata": {},
   "source": [
    "### Optimisation de l'architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SmallConvCVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes=10): \n",
    "        super(SmallConvCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # --- ENCODER PLUS PETIT ---\n",
    "        # On divise par 2 le nombre de filtres : (16, 32, 64) au lieu de (32, 64, 128)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1 + num_classes, 16, kernel_size=4, stride=2, padding=1), # Output: (16, 14, 14)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1), # Output: (32, 7, 7)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Output: (64, 4, 4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Transition vers l'espace latent avec la dimension réduite (64 * 4 * 4)\n",
    "        self.fc_mu = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "\n",
    "        # --- DECODER PLUS PETIT ---\n",
    "        self.fc_decode = nn.Linear(latent_dim + num_classes, 64 * 4 * 4) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1), # Output: (32, 7, 7)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # Output: (16, 14, 14)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1), # Output: (1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c): \n",
    "        # Étirement du label pour la concaténation spatiale\n",
    "        c_expanded = c.view(-1, self.num_classes, 1, 1).expand(-1, -1, 28, 28)\n",
    "        inputs = torch.cat([x, c_expanded], 1)\n",
    "        \n",
    "        x = self.encoder(inputs)\n",
    "        x = x.view(-1, 64 * 4 * 4) # Mise à jour pour 64 filtres\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        inputs = torch.cat([z, c], 1) \n",
    "        x = self.fc_decode(inputs)\n",
    "        x = x.view(-1, 64, 4, 4) # Mise à jour pour 64 filtres\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # 1. Création du One-Hot Vector (Logique identique au modèle normal)\n",
    "        c = torch.zeros(x.size(0), self.num_classes).to(x.device)\n",
    "        c.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # 2. Encodage\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        \n",
    "        # 3. Sampling\n",
    "        z = self.sample(mu, logvar)\n",
    "        \n",
    "        # 4. Décodage\n",
    "        recon_x = self.decode(z, c)\n",
    "        \n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1c196",
   "metadata": {},
   "source": [
    "#### Comparaison des deux architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4143cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparamètres optimaux ---\n",
    "best_config = {\n",
    "    'latent_dim': 10,\n",
    "    'beta': 1.0,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "# --- DataLoaders ---\n",
    "train_loader_comp = torch.utils.data.DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "val_loader_comp = torch.utils.data.DataLoader(val_dataset, batch_size=best_config['batch_size'], shuffle=False)\n",
    "\n",
    "# --- Initialisation ---\n",
    "models = {\n",
    "    'Normal': ConvCVAE(latent_dim=best_config['latent_dim'], num_classes=10).to(device),\n",
    "    'Petit': SmallConvCVAE(latent_dim=best_config['latent_dim'], num_classes=10).to(device)\n",
    "}\n",
    "\n",
    "# Dictionnaires pour stocker les scores de validation\n",
    "val_history = {'Normal': [], 'Petit': []}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Évaluation architecture : {name} ---\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_config['lr'])\n",
    "    \n",
    "    for epoch in range(1, 21):\n",
    "        # Phase d'entraînement\n",
    "        model.train()\n",
    "        for data, labels in train_loader_comp:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data, labels)\n",
    "            loss, _, _ = loss_function(recon, data, mu, logvar, beta=best_config['beta'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Phase de Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader_comp:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                recon, mu, logvar = model(data, labels)\n",
    "                # Calcul de la loss sur le set de validation\n",
    "                loss, _, _ = loss_function(recon, data, mu, logvar, beta=best_config['beta'])\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / len(val_dataset)\n",
    "        val_history[name].append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# --- Visualisation des résultats de validation ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_history['Normal'], label=\"Normal (Validation Loss)\", marker='o')\n",
    "plt.plot(val_history['Petit'], label=\"Petit (Validation Loss)\", marker='s')\n",
    "plt.title(\"Comparaison de la Performance sur le Jeu de Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Total Loss (BCE + β*KLD)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e552b53",
   "metadata": {},
   "source": [
    "La strcuture normale du CVAE est plus efficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea16ae3",
   "metadata": {},
   "source": [
    "### Check sur-apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "best_config = {\n",
    "    'latent_dim': 10,\n",
    "    'beta': 1.0,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128\n",
    "}\n",
    "epochs = 20\n",
    "\n",
    "# Préparation des DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=best_config['batch_size'], shuffle=False)\n",
    "\n",
    "# Initialisation du modèle normal\n",
    "model = ConvCVAE(latent_dim=best_config['latent_dim'], num_classes=10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_config['lr'])\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Début de l'analyse du surapprentissage...\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # --- Phase d'Entraînement ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon, mu, logvar = model(data, labels)\n",
    "        loss, _, _ = loss_function(recon, data, mu, logvar, beta=best_config['beta'])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # --- Phase de Validation ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            recon, mu, logvar = model(data, labels)\n",
    "            loss, _, _ = loss_function(recon, data, mu, logvar, beta=best_config['beta'])\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# --- Visualisation des courbes d'apprentissage ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Loss Entraînement', color='blue', linewidth=2)\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Loss Validation', color='red', linestyle='--', linewidth=2)\n",
    "plt.title(\"Courbes d'apprentissage : Vérification du surapprentissage\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Totale Moyenne (BCE + β*KLD)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065578b5",
   "metadata": {},
   "source": [
    "Pas de surapprentisagge visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dae22",
   "metadata": {},
   "source": [
    "## Test sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test_evaluation(model, test_loader, device, beta):\n",
    "    model.eval()\n",
    "    test_bce = 0\n",
    "    test_kld = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    # Pour la visualisation\n",
    "    originals = []\n",
    "    reconstructions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon, mu, logvar = model(data, labels)\n",
    "            \n",
    "            # Calcul de la perte\n",
    "            loss, bce, kld = loss_function(recon, data, mu, logvar, beta)\n",
    "            \n",
    "            test_total += loss.item()\n",
    "            test_bce += bce.item()\n",
    "            test_kld += kld.item()\n",
    "            \n",
    "            # Garder quelques exemples pour l'affichage\n",
    "            if i == 0:\n",
    "                originals = data.cpu()\n",
    "                reconstructions = recon.cpu()\n",
    "\n",
    "    # Moyennes\n",
    "    n_test = len(test_loader.dataset)\n",
    "    print(f\"--- RÉSULTATS FINAUX (TEST SET) ---\")\n",
    "    print(f\"Total Loss : {test_total / n_test:.4f}\")\n",
    "    print(f\"BCE (Reconstruction) : {test_bce / n_test:.4f}\")\n",
    "    print(f\"KLD (Régularisation) : {test_kld / n_test:.4f}\")\n",
    "    \n",
    "    return originals, reconstructions\n",
    "\n",
    "# Exécution\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "orig, rec = final_test_evaluation(model, test_loader, device, best_config['beta'])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(8):\n",
    "    # Originaux\n",
    "    plt.subplot(2, 8, i + 1)\n",
    "    plt.imshow(orig[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0: plt.ylabel(\"Original\")\n",
    "    \n",
    "    # Reconstructions\n",
    "    plt.subplot(2, 8, i + 9)\n",
    "    plt.imshow(rec[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0: plt.ylabel(\"Reconstruit\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbe5f8",
   "metadata": {},
   "source": [
    "partie fonction de perte à optimiser - partie a reprendre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72782688",
   "metadata": {},
   "outputs": [],
   "source": [
    " # On reprend ton modèle standard\n",
    "final_model = ConvCVAE(latent_dim=2).to(device)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=1e-3)\n",
    "\n",
    "history_bce = []\n",
    "history_kld = []\n",
    "epochs_loss = 10 # Suffisant pour voir la dynamique\n",
    "\n",
    "print(\"--- Analyse des composantes de la Loss ---\")\n",
    "\n",
    "for epoch in range(epochs_loss):\n",
    "    final_model.train()\n",
    "    epoch_bce = 0\n",
    "    epoch_kld = 0\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = final_model(data, labels)\n",
    "        \n",
    "        # On récupère les composants séparés\n",
    "        loss, bce, kld = loss_function(recon, data, mu, logvar, beta=1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_bce += bce.item()\n",
    "        epoch_kld += kld.item()\n",
    "        \n",
    "    history_bce.append(epoch_bce / len(train_loader))\n",
    "    history_kld.append(epoch_kld / len(train_loader))\n",
    "\n",
    "# Plot avec deux échelles (car BCE est très grand et KLD très petit)\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Reconstruction Loss (BCE)', color=color)\n",
    "ax1.plot(history_bce, color=color, label='BCE (Qualité Image)')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # Instancier un deuxième axe y\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('KL Divergence (KLD)', color=color)\n",
    "ax2.plot(history_kld, color=color, linestyle='--', label='KLD (Régularisation)')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"Dynamique d'apprentissage : BCE vs KLD\")\n",
    "fig.tight_layout()\n",
    "plt.show()##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
