{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293681e2",
   "metadata": {},
   "source": [
    "# Mini-projet n°1 – Chats ou Chiens ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Maths - Stats\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "# Data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Deep Learning Librairies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout,  GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, clone_model, Model,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./oxford-iiit-pet/annotations/list.txt\",\n",
    "    sep=r\"\\s+\",                                         \n",
    "    comment=\"#\",                                        \n",
    "    header=None,                                        \n",
    "    names=[\"Image\", \"CLASS_ID\", \"SPECIES\", \"BREED_ID\"]\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa222c9",
   "metadata": {},
   "source": [
    "Le jeu de données est composé de 7349 individus.\n",
    "Les variables sont les suivantes :\n",
    "- Image : au format .jpg ;\n",
    "- CLASS\\_ID (race) : 37 races différentes ;\n",
    "- SPECIES : chat ou chien (1 ou 2 selon l'espèce) ;\n",
    "- BREED\\_ID : identifiant de la race parmi l'espèce concernée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2aab5f",
   "metadata": {},
   "source": [
    "Afin de rendre les données plus lisibles, nous rajoutons deux variables non numériques précisant les noms de l'espèce (SPECIES\\_NAME) et de la race (BREED\\_NAME) de chaque individu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BREED_NAME\"] = df[\"Image\"].str.rsplit(\"_\", n=1).str[0]           # Nom de la race\n",
    "df[\"SPECIES_NAME\"] = df[\"SPECIES\"].map({1: \"Cat\", 2: \"Dog\"})         # Chat ou chien\n",
    "df[\"Image\"] = df[\"Image\"].apply(lambda x: f\"{x}.jpg\")\n",
    "\n",
    "print(df.sample(n=3, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts = df[\"SPECIES_NAME\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    species_counts.values,\n",
    "    labels=species_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90\n",
    ")\n",
    "plt.title(\"Répartition des individus par espèce\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6b6a9",
   "metadata": {},
   "source": [
    "67.7 % des individus du jeu de données sont des chiens et 32.3% sont des chats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    df,\n",
    "    index=\"BREED_NAME\",\n",
    "    columns=\"SPECIES_NAME\",\n",
    "    values=\"Image\",\n",
    "    aggfunc=\"count\"\n",
    ")\n",
    "\n",
    "pivot.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(14, 6)\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Nombre d'individus\")\n",
    "plt.title(\"Nombre d'individus par race et par espèce\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cdb2c",
   "metadata": {},
   "source": [
    "Le nombre d'individus est environ le même pour chaque race (200). Les données semblent a priori équilibré,  même si certaines races (Bombay, chat) n'ont pas autant d'individus que les autres.\n",
    "On peut conclure cette sous-partie en avançant que les données sont suffisament bien réparties selon les espèces et les races. Il y a une légère sur-représentations des chiens, et les races sont légèrement déséquilibrés, mais cela ne devrait pas être impactant pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd17213",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = os.path.join(df, \"images/\")\n",
    "mask_dir = os.path.join(df, \"annotations/trimaps/\")\n",
    "\n",
    "# --- Ajout de la colonne des masques ---\n",
    "# Les masques portent le même nom que l'image, mais en .png\n",
    "# Votre colonne 'Image' contient déjà l'extension .jpg, on doit la remplacer\n",
    "df['mask_filename'] = df['Image'].str.replace('.jpg', '.png')\n",
    "\n",
    "# Vérification que les fichiers existent (optionnel mais recommandé)\n",
    "# df = df[df['Image'].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))]\n",
    "\n",
    "# --- Division du Dataset (Train / Val / Test) ---\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['SPECIES_NAME'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['SPECIES_NAME'])\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Validation: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc5a9a",
   "metadata": {},
   "source": [
    "### Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07801718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Paramètres ---\n",
    "IMG_SIZE = (128, 128) # Réduit pour soulager le CPU/GPU\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Générateurs (Data Augmentation vue en TP) ---\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# --- Création du flux pour les RACES (Classification Fine) ---\n",
    "# Pour Chats vs Chiens, changez juste y_col=\"SPECIES_NAME\" et class_mode=\"binary\"\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=img_dir,\n",
    "    x_col=\"Image\",\n",
    "    y_col=\"SPECIES_NAME\",      # Cible : Nom de la race\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\" # 37 classes\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=img_dir,\n",
    "    x_col=\"Image\",\n",
    "    y_col=\"SPECIES_NAME\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# --- Modèle Transfer Learning (VGG16) ---\n",
    "# Comme vu dans la section \"Pre-trained Network\" du TP\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SIZE + (3,))\n",
    "base_model.trainable = False  # On gèle les poids\n",
    "\n",
    "# Construction du classifier\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # Ou Flatten() comme dans le TP simple\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(37, activation='softmax')(x) # 37 neurones pour les 37 races\n",
    "\n",
    "model_clf = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model_clf.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# model_clf.summary()\n",
    "# history_clf = model_clf.fit(train_generator, validation_data=val_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction d'analyse des résultats (Train vs Val + Test Score)\n",
    "def analyser_resultats(model, history, test_generator, model_name=\"Modèle\"):\n",
    "    # 1. Courbes d'apprentissage\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Précision\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'{model_name} - Précision')\n",
    "\n",
    "    # Perte (Loss)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'{model_name} - Perte')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Évaluation sur le jeu de Test\n",
    "    print(f\"--- Évaluation finale sur le Test Set ({model_name}) ---\")\n",
    "    test_loss, test_acc = model.evaluate(test_generator)\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62525d4d",
   "metadata": {},
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42867056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateurs simples (juste mise à l'échelle)\n",
    "simple_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_simple = simple_datagen.flow_from_dataframe(\n",
    "    train_df, directory=img_dir, x_col='Image', y_col='BREED_NAME',\n",
    "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\n",
    ")\n",
    "val_gen_simple = simple_datagen.flow_from_dataframe(\n",
    "    val_df, directory=img_dir, x_col='Image', y_col='BREED_NAME',\n",
    "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\n",
    ")\n",
    "test_gen_simple = simple_datagen.flow_from_dataframe(\n",
    "    test_df, directory=img_dir, x_col='Image', y_col='BREED_NAME',\n",
    "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Architecture CNN Simple (Style AlexNet simplifié comme dans le TP)\n",
    "def build_simple_cnn():\n",
    "    model = Sequential([\n",
    "        Input(shape=IMG_SIZE + (3,)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax') # 37 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Entraînement et Analyse\n",
    "model_simple = build_simple_cnn()\n",
    "history_simple = model_simple.fit(train_gen_simple, validation_data=val_gen_simple, epochs=10)\n",
    "\n",
    "analyser_resultats(model_simple, history_simple, test_gen_simple, \"CNN Simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0abe2f",
   "metadata": {},
   "source": [
    "data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateur avec Augmentation (Rotation, Zoom, Flip...)\n",
    "aug_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Seul le train est augmenté ! Val et Test restent \"normaux\"\n",
    "train_gen_aug = aug_datagen.flow_from_dataframe(\n",
    "    train_df, directory=img_dir, x_col='Image', y_col='BREED_NAME',\n",
    "    target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\n",
    ")\n",
    "\n",
    "# On réutilise le même modèle, mais entraîné sur des données augmentées\n",
    "model_aug = build_simple_cnn() # On repart de zéro\n",
    "history_aug = model_aug.fit(train_gen_aug, validation_data=val_gen_simple, epochs=15) # + d'epochs nécessaires\n",
    "\n",
    "analyser_resultats(model_aug, history_aug, test_gen_simple, \"CNN + Data Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba9e06",
   "metadata": {},
   "source": [
    "pre trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de VGG16 sans la partie classification (include_top=False)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SIZE + (3,))\n",
    "base_model.trainable = False  # IMPORTANT : On gèle les poids du VGG\n",
    "\n",
    "# Ajout de notre tête de classification\n",
    "inputs = Input(shape=IMG_SIZE + (3,))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x) # Plus efficace que Flatten\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model_transfer = Model(inputs, outputs)\n",
    "model_transfer.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement (Rapide car on n'entraîne que les dernières couches)\n",
    "history_transfer = model_transfer.fit(train_gen_aug, validation_data=val_gen_simple, epochs=10)\n",
    "\n",
    "analyser_resultats(model_transfer, history_transfer, test_gen_simple, \"Transfer Learning (VGG16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c72044",
   "metadata": {},
   "source": [
    "fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dégeler le modèle de base\n",
    "base_model.trainable = True\n",
    "\n",
    "# 2. On gèle les premières couches (garder les features génériques) et on laisse les dernières libres\n",
    "# VGG16 a ~19 layers. On fine-tune à partir de la couche 15 par exemple.\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3. Re-compiler avec un learning rate TRÈS faible (pour ne pas tout casser)\n",
    "model_transfer.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Continuer l'entraînement\n",
    "history_finetune = model_transfer.fit(train_gen_aug, validation_data=val_gen_simple, epochs=10)\n",
    "\n",
    "analyser_resultats(model_transfer, history_finetune, test_gen_simple, \"Fine Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922fbd8a",
   "metadata": {},
   "source": [
    "### Classification fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Paramètres ---\n",
    "IMG_SIZE = (128, 128) # Réduit pour soulager le CPU/GPU\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Générateurs (Data Augmentation vue en TP) ---\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# --- Création du flux pour les RACES (Classification Fine) ---\n",
    "# Pour Chats vs Chiens, changez juste y_col=\"SPECIES_NAME\" et class_mode=\"binary\"\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=img_dir,\n",
    "    x_col=\"Image\",\n",
    "    y_col=\"BREED_NAME\",      # Cible : Nom de la race\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\" # 37 classes\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=img_dir,\n",
    "    x_col=\"Image\",\n",
    "    y_col=\"BREED_NAME\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# --- Modèle Transfer Learning (VGG16) ---\n",
    "# Comme vu dans la section \"Pre-trained Network\" du TP\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=IMG_SIZE + (3,))\n",
    "base_model.trainable = False  # On gèle les poids\n",
    "\n",
    "# Construction du classifier\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # Ou Flatten() comme dans le TP simple\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(37, activation='softmax')(x) # 37 neurones pour les 37 races\n",
    "\n",
    "model_clf = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model_clf.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# model_clf.summary()\n",
    "# history_clf = model_clf.fit(train_generator, validation_data=val_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716896e",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PetSegmentationGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, img_dir, mask_dir, batch_size=32, img_size=(128, 128)):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.df[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            # 1. Charger l'image\n",
    "            img_path = os.path.join(self.img_dir, row['Image'])\n",
    "            img = load_img(img_path, target_size=self.img_size)\n",
    "            img = img_to_array(img) / 255.0 # Normalisation\n",
    "            \n",
    "            # 2. Charger le masque\n",
    "            mask_path = os.path.join(self.mask_dir, row['mask_filename'])\n",
    "            # load_img en grayscale pour avoir (H, W) et pas (H, W, 3)\n",
    "            mask = load_img(mask_path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            mask = img_to_array(mask)\n",
    "            \n",
    "            # 3. Prétraitement du masque Oxford\n",
    "            # Valeurs originales : 1 (Animal), 2 (Fond), 3 (Bordure)\n",
    "            # On transforme en Binaire : 1 (Animal) vs 0 (Le reste)\n",
    "            mask = np.where(mask == 1, 1.0, 0.0)\n",
    "            \n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "            \n",
    "        return np.array(images), np.array(masks)\n",
    "\n",
    "# Création des générateurs de segmentation\n",
    "train_gen_seg = PetSegmentationGenerator(train_df, img_dir, mask_dir, BATCH_SIZE, IMG_SIZE)\n",
    "val_gen_seg = PetSegmentationGenerator(val_df, img_dir, mask_dir, BATCH_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1084d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # --- Contraction (Encoder) ---\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    # --- Bottleneck ---\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    # --- Expansion (Decoder) ---\n",
    "    u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u5 = Concatenate()([u5, c3]) # Skip connection\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = Concatenate()([u6, c2])\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = Concatenate()([u7, c1])\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    # --- Sortie ---\n",
    "    # Sigmoid car on fait une segmentation binaire (pixel = animal ou pas)\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "unet_model = build_unet(IMG_SIZE + (3,))\n",
    "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# unet_model.summary()\n",
    "# history_unet = unet_model.fit(train_gen_seg, validation_data=val_gen_seg, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba59ee",
   "metadata": {},
   "source": [
    "### Pour aller plus loin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a288fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Conv2DTranspose, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_multitask_model(input_shape, num_classes):\n",
    "    inputs = Input(input_shape, name='input_image')\n",
    "\n",
    "    # --- 1. ENCODEUR PARTAGÉ (Shared Backbone) ---\n",
    "    # On peut utiliser un VGG16 pré-entraîné ou faire le nôtre\n",
    "    # Ici, un exemple simple \"From Scratch\" pour bien comprendre\n",
    "    \n",
    "    # Block 1\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x) # 64x64\n",
    "    f1 = x # Skip connection pour la segmentation\n",
    "    \n",
    "    # Block 2\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x) # 32x32\n",
    "    f2 = x \n",
    "    \n",
    "    # Block 3 (Bottleneck partagé)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2))(x) # 16x16\n",
    "    \n",
    "    # --- 2. BRANCHE CLASSIFICATION (Tête 1) ---\n",
    "    c = GlobalAveragePooling2D()(encoded) # Aplatit les features\n",
    "    c = Dense(128, activation='relu')(c)\n",
    "    c = Dropout(0.5)(c)\n",
    "    # Sortie 1 : Classification (Nommé 'class_output')\n",
    "    class_output = Dense(num_classes, activation='softmax', name='class_output')(c)\n",
    "\n",
    "    # --- 3. BRANCHE SEGMENTATION (Tête 2 - Decoder) ---\n",
    "    # On remonte la résolution comme dans un U-Net\n",
    "    \n",
    "    # Upsample 1\n",
    "    s = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(encoded) # 32x32\n",
    "    s = Concatenate()([s, f2]) # On récupère l'info spatiale de l'encodeur\n",
    "    s = Conv2D(64, (3, 3), activation='relu', padding='same')(s)\n",
    "    \n",
    "    # Upsample 2\n",
    "    s = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(s) # 64x64\n",
    "    s = Concatenate()([s, f1])\n",
    "    s = Conv2D(32, (3, 3), activation='relu', padding='same')(s)\n",
    "    \n",
    "    # Upsample 3 (Retour taille originale)\n",
    "    s = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(s) # 128x128\n",
    "    \n",
    "    # Sortie 2 : Segmentation (Nommé 'seg_output')\n",
    "    seg_output = Conv2D(1, (1, 1), activation='sigmoid', name='seg_output')(s)\n",
    "\n",
    "    # --- MODÈLE FINAL ---\n",
    "    model = Model(inputs=inputs, outputs=[class_output, seg_output])\n",
    "    return model\n",
    "\n",
    "# Création du modèle\n",
    "model_joint = build_multitask_model((128, 128, 3), num_classes=37)\n",
    "model_joint.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c91049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, to_categorical\n",
    "\n",
    "class MultiTaskGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, img_dir, mask_dir, batch_size, img_size, num_classes):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Création d'un mapping race -> int si ce n'est pas déjà fait\n",
    "        self.breeds = sorted(df['BREED_NAME'].unique())\n",
    "        self.breed_to_idx = {b: i for i, b in enumerate(self.breeds)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.df[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        \n",
    "        images = []\n",
    "        class_labels = []\n",
    "        masks = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Image\n",
    "            img = load_img(os.path.join(self.img_dir, row['Image']), target_size=self.img_size)\n",
    "            images.append(img_to_array(img) / 255.0)\n",
    "            \n",
    "            # Label Classification (One-hot encoding)\n",
    "            label_idx = self.breed_to_idx[row['BREED_NAME']]\n",
    "            class_labels.append(label_idx)\n",
    "            \n",
    "            # Masque Segmentation\n",
    "            mask_path = os.path.join(self.mask_dir, row['mask_filename'])\n",
    "            mask = load_img(mask_path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            mask = img_to_array(mask)\n",
    "            mask = np.where(mask == 1, 1.0, 0.0) # Binarisation\n",
    "            masks.append(mask)\n",
    "            \n",
    "        X = np.array(images)\n",
    "        y_class = to_categorical(class_labels, num_classes=self.num_classes)\n",
    "        y_seg = np.array(masks)\n",
    "        \n",
    "        # IMPORTANT : On renvoie un dictionnaire correspondant aux noms des couches de sortie\n",
    "        return X, {'class_output': y_class, 'seg_output': y_seg}\n",
    "\n",
    "# Instanciation\n",
    "train_gen_multi = MultiTaskGenerator(train_df, img_dir, mask_dir, 32, (128, 128), 37)\n",
    "val_gen_multi = MultiTaskGenerator(val_df, img_dir, mask_dir, 32, (128, 128), 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69318923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit une loss différente pour chaque sortie\n",
    "losses = {\n",
    "    'class_output': 'categorical_crossentropy',\n",
    "    'seg_output': 'binary_crossentropy' # Ou une Dice Loss personnalisée\n",
    "}\n",
    "\n",
    "# On peut donner plus d'importance à l'une ou l'autre\n",
    "loss_weights = {\n",
    "    'class_output': 1.0,  # Poids de la classification\n",
    "    'seg_output': 1.0     # Poids de la segmentation\n",
    "}\n",
    "\n",
    "model_joint.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics={\n",
    "        'class_output': 'accuracy', \n",
    "        'seg_output': 'accuracy' # Ou IoU\n",
    "    }\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "history = model_joint.fit(\n",
    "    train_gen_multi,\n",
    "    validation_data=val_gen_multi,\n",
    "    epochs=15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDDLtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
